[
["index.html", "Deep Learning from Scratch in R Preface", " Deep Learning from Scratch in R Kota Mori 30 May, 2017 Preface We will implement Deep Learing from Scratch in R. This is an inofficial self-study project. GitHub Repository "],
["introduction-to-r-and-some-remarks-on-differences-from-python.html", "1 Introduction to R, and some remarks on differences from Python 1.1 Data types 1.2 Variables 1.3 List and vectors 1.4 Dictionary, and hash table 1.5 Boolean or logicals 1.6 If-statement 1.7 For loop 1.8 Functions 1.9 Script File 1.10 Class 1.11 Matrix and array 1.12 Broadcast 1.13 Array 1.14 Access matrix/array elements 1.15 Plot 1.16 Image plot", " 1 Introduction to R, and some remarks on differences from Python This chapter corresponds to Cahpter 1, “Introruction to Python” in the original book. 1.1 Data types We will use class function to know the variable types. Notice that type of 10 is numeric not integer. Append L to make it treated as an integer. class(10) ## [1] &quot;numeric&quot; class(10L) ## [1] &quot;integer&quot; class(2.718) ## [1] &quot;numeric&quot; class(&quot;hello&quot;) ## [1] &quot;character&quot; 1.2 Variables Like Python, R also supports dynamic typing. x &lt;- 100L y &lt;- 3.14 x*y ## [1] 314 class(x*y) ## [1] &quot;numeric&quot; 1.3 List and vectors For basic types such as numeric and character, we can make a vector by c function. R uses one-base indexing (index starts with 1, not 0). a &lt;- c(1,2,3,4,5) a ## [1] 1 2 3 4 5 length(a) ## [1] 5 a[1] ## [1] 1 a[5] ## [1] 5 a[5] &lt;- 99 a ## [1] 1 2 3 4 99 What is more similar to python’s list would be list, since it can have different types of elements. Use [[ to access list elements. a &lt;- list(1,2,3,4,5) a ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 5 length(a) ## [1] 5 a[[1]] ## [1] 1 a[[5]] ## [1] 5 a[[5]] &lt;- 99 a ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 99 1.4 Dictionary, and hash table Vectors and lists can have names, with which we can use them like a dictionary. ## Vector me &lt;- c(height = 180) me[&quot;height&quot;] ## height ## 180 me[&quot;weight&quot;] &lt;- 70 me ## height weight ## 180 70 ## List me &lt;- list(height = 180) me$height ## [1] 180 me$weight &lt;- 70 me ## $height ## [1] 180 ## ## $weight ## [1] 70 Alternatively, we can use hash function from hash library. library(hash) me &lt;- hash(height = 180) me$height ## $height ## [1] 180 me$weight &lt;- 70 me ## &lt;hash&gt; containing 2 key-value pair(s). ## height : 180 ## weight : 70 1.5 Boolean or logicals In R, boolean variables are called logical and have values TRUE or FALSE. hungry &lt;- TRUE sleepy &lt;- FALSE class(hungry) ## [1] &quot;logical&quot; !hungry ## [1] FALSE hungry &amp; sleepy ## [1] FALSE hungry | sleepy ## [1] TRUE 1.6 If-statement hungry &lt;- TRUE if (hungry) print(&quot;I&#39;m hungry&quot;) ## [1] &quot;I&#39;m hungry&quot; hungry &lt;- FALSE if (hungry) { print(&quot;I&#39;m hungry&quot;) } else { print(&quot;I&#39;m not hungry&quot;) print(&quot;I&#39;m sleepy&quot;) } ## [1] &quot;I&#39;m not hungry&quot; ## [1] &quot;I&#39;m sleepy&quot; 1.7 For loop for (i in 1:3) print(i) ## [1] 1 ## [1] 2 ## [1] 3 1.8 Functions hello &lt;- function() { print(&quot;Hello World!&quot;) } hello() ## [1] &quot;Hello World!&quot; hello &lt;- function(object) { print(sprintf(&quot;Hello %s!&quot;, object)) } hello(&quot;cat&quot;) ## [1] &quot;Hello cat!&quot; 1.9 Script File To run a script file from R session, use source function. source(&quot;hungry.R&quot;) ## [1] &quot;I&#39;m hungry!&quot; To run a script file from console, call Rscript. $ Rscript hungry.R [1] &quot;I&#39;m hungry!&quot; 1.10 Class R has various ways to define classes. In this exercise, we will use R6 implementation. See more about object oriented programming in R, see Hadley’s Advanced R. library(R6) Man &lt;- R6Class( &quot;Man&quot;, public = list( name = &quot;&quot;, initialize = function(name) { self$name &lt;- name }, hello = function() { print(sprintf(&quot;Hello %s!&quot;, self$name)) }, goodbye = function() { print(sprintf(&quot;Gooe-bye %s!&quot;, self$name)) } ) ) m &lt;- Man$new(&quot;David&quot;) m$hello() ## [1] &quot;Hello David!&quot; m$goodbye() ## [1] &quot;Gooe-bye David!&quot; 1.11 Matrix and array What’s similar to numpy’s arrays in R is vector (1d), matrix (2d) and array (3d+). x &lt;- c(1,2,3) y &lt;- c(2,4,6) x+y ## [1] 3 6 9 x*y ## [1] 2 8 18 x/y ## [1] 0.5 0.5 0.5 x/2 ## [1] 0.5 1.0 1.5 A &lt;- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=TRUE) A ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 dim(A) ## [1] 2 2 B &lt;- matrix(c(3,0,0,6), nrow=2, ncol=2, byrow=TRUE) A+B ## [,1] [,2] ## [1,] 4 2 ## [2,] 3 10 A*B ## [,1] [,2] ## [1,] 3 0 ## [2,] 0 24 A*10 ## [,1] [,2] ## [1,] 10 20 ## [2,] 30 40 1.12 Broadcast R’s broadcast rule is the opposite of that of python. A &lt;- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=TRUE) B &lt;- c(10,20) A*B ## [,1] [,2] ## [1,] 10 20 ## [2,] 60 80 # to reproduce the book&#39;s result, A &lt;- matrix(c(1,2,3,4), nrow=2, ncol=2, byrow=FALSE) B &lt;- c(10,20) t(A*B) ## [,1] [,2] ## [1,] 10 40 ## [2,] 30 80 1.13 Array x &lt;- array(1, dim=c(4,3,2)) x ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 ## [4,] 1 1 1 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 ## [4,] 1 1 1 y &lt;- 1:8 x*y ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 5 1 ## [2,] 2 6 2 ## [3,] 3 7 3 ## [4,] 4 8 4 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 5 1 5 ## [2,] 6 2 6 ## [3,] 7 3 7 ## [4,] 8 4 8 1.14 Access matrix/array elements X &lt;- matrix(c(51,55,14,19,0,4), nrow=3, ncol=2, byrow=TRUE) X ## [,1] [,2] ## [1,] 51 55 ## [2,] 14 19 ## [3,] 0 4 X[1,] ## [1] 51 55 X[1,2] ## [1] 55 for (i in 1:nrow(X)) print(X[i,]) ## [1] 51 55 ## [1] 14 19 ## [1] 0 4 Elements are tracked by “column” first way in R. So flattening behavior is different from Python. dim(X) &lt;- prod(dim(X)) X ## [1] 51 14 0 55 19 4 # to reproduce the book result, X &lt;- matrix(c(51,55,14,19,0,4), nrow=3, ncol=2, byrow=FALSE) dim(X) &lt;- prod(dim(X)) X ## [1] 51 55 14 19 0 4 X[c(1,3,5)] ## [1] 51 14 0 X &gt; 15 ## [1] TRUE TRUE FALSE TRUE FALSE FALSE X[X &gt; 15] ## [1] 51 55 19 1.15 Plot R has various ways to visualize data. In this exercise, we will use ggplot2 library. library(ggplot2) x &lt;- seq(0, 6, 0.1) y &lt;- sin(x) qplot(x, y, geom=&quot;line&quot;) y1 &lt;- sin(x) y2 &lt;- cos(x) tmp &lt;- rbind(data.frame(x=x, y=y1, func=&quot;sin&quot;, stringsAsFactors=FALSE), data.frame(x=x, y=y2, func=&quot;cos&quot;, stringsAsFactors=FALSE)) ggplot(tmp, aes(x, y, linetype=func)) + geom_line() + ggtitle(&quot;Sin &amp; Cos&quot;) 1.16 Image plot To read a png format image file, use readPNG from png library. It is loaded as a 3d array of shape (nrow, ncol, channel). To visualize image, we can use grid.raster from grid library. library(png) img &lt;- readPNG(&quot;dataset/lena.png&quot;) str(img) ## num [1:256, 1:256, 1:3] 0.875 0.882 0.89 0.89 0.878 ... library(grid) grid.raster(img) grid.raster does not return graphic object. To use image plot as if it is a ggplot object, we will use the following custom function. image_plot &lt;- function(arr) { out &lt;- grid::rasterGrob(arr) out &lt;- ggplot2::qplot(0.5, 0.5, xlim=c(0,1), ylim=c(0,1)) + ggplot2::theme_void() + ggplot2::xlab(&#39;&#39;) + ggplot2::ylab(&#39;&#39;) + ggplot2::annotation_custom(out) out } image_plot(img) class(image_plot(img)) ## [1] &quot;gg&quot; &quot;ggplot&quot; "],
["perceptron.html", "2 Perceptron", " 2 Perceptron This chapter corresponds to Chapter 2, “Perceptron” in the original book. AND &lt;- function(x1, x2) { w1 &lt;- 0.5 w2 &lt;- 0.5 b &lt;- -0.7 tmp &lt;- x1*w1 + x2*w2 + b if (tmp &lt;= 0) 0 else 1 } cat(AND(0,0), AND(1,0), AND(0,1), AND(1,1), &quot;\\n&quot;) ## 0 0 0 1 NAND &lt;- function(x1, x2) { w1 &lt;- -0.5 w2 &lt;- -0.5 b &lt;- 0.7 tmp &lt;- x1*w1 + x2*w2 + b if (tmp &lt;= 0) 0 else 1 } cat(NAND(0,0), NAND(1,0), NAND(0,1), NAND(1,1), &quot;\\n&quot;) ## 1 1 1 0 OR &lt;- function(x1, x2) { w1 &lt;- 0.5 w2 &lt;- 0.5 b &lt;- -0.2 tmp &lt;- x1*w1 + x2*w2 + b if (tmp &lt;= 0) 0 else 1 } cat(OR(0,0), OR(1,0), OR(0,1), OR(1,1), &quot;\\n&quot;) ## 0 1 1 1 XOR &lt;- function(x1, x2) { s1 &lt;- NAND(x1, x2) s2 &lt;- OR(x1, x2) AND(s1, s2) } cat(XOR(0,0), XOR(1,0), XOR(0,1), XOR(1,1), &quot;\\n&quot;) ## 0 1 1 0 "],
["neural-network.html", "3 Neural Network 3.1 Activation functions 3.2 Array manipulation 3.3 Output layer 3.4 MNIST", " 3 Neural Network This chapter corresponds to Chapter 3, “Neural Network” in the original book. 3.1 Activation functions 3.1.1 Step function library(ggplot2) step_func &lt;- function(a) { as.integer(a &gt; 0) } x &lt;- seq(-5, 5, 0.1) y &lt;- step_func(x) qplot(x, y, geom=&quot;line&quot;) 3.1.2 Sigmoid function sigmoid &lt;- function(a) { 1 / (1 + exp(-a)) } x &lt;- c(-1, 1, 2) sigmoid(x) ## [1] 0.2689414 0.7310586 0.8807971 x &lt;- seq(-5, 5, 0.1) y &lt;- sigmoid(x) qplot(x, y, geom=&quot;line&quot;) 3.1.3 Rectified Linear Unit (ReLU) relu &lt;- function(a) { pmax(0, a) } x &lt;- seq(-5, 5, 0.1) y &lt;- relu(x) qplot(x, y, geom=&quot;line&quot;) 3.2 Array manipulation Python’s shape corresponds to R’s dim function. There is no ndim counterpart in R; Use length(dim(A)). We use array instead of c to make a 1d-array. Doing so we can apply dim function. A &lt;- array(1:4) length(dim(A)) ## [1] 1 dim(A) ## [1] 4 B &lt;- matrix(c(1:6), nrow=3, ncol=2, byrow=TRUE) B ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 length(dim(B)) ## [1] 2 dim(B) ## [1] 3 2 3.2.1 Dot product Dot product is calculated by the %*% operator. A &lt;- matrix(1:4, nrow=2, ncol=2, byrow=TRUE) dim(A) ## [1] 2 2 B &lt;- matrix(5:8, nrow=2, ncol=2, byrow=TRUE) dim(B) ## [1] 2 2 A %*% B ## [,1] [,2] ## [1,] 19 22 ## [2,] 43 50 A &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) dim(A) ## [1] 2 3 B &lt;- matrix(1:6, nrow=3, ncol=2, byrow=TRUE) dim(B) ## [1] 3 2 A %*% B ## [,1] [,2] ## [1,] 22 28 ## [2,] 49 64 Also convenient would be crossprod and tcrossprod functions. crossprod(A, b) = t(A) %*% B and tcrossprod(A, B) = A %*% t(B). crossprod(t(A), B) ## [,1] [,2] ## [1,] 22 28 ## [2,] 49 64 tcrossprod(A, t(B)) ## [,1] [,2] ## [1,] 22 28 ## [2,] 49 64 Size mismatch raises an error. A &lt;- matrix(1:6, nrow=2, ncol=3, byrow=TRUE) dim(A) ## [1] 2 3 C &lt;- matrix(1:4, nrow=2, ncol=2, byrow=TRUE) dim(C) ## [1] 2 2 A %*% C ## Error in A %*% C: non-conformable arguments 3.2.2 Dot products in neural network X &lt;- array(1:2) dim(X) ## [1] 2 W &lt;- matrix(1:6, nrow=2, ncol=3, byrow=FALSE) W ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 dim(W) ## [1] 2 3 X %*% W ## [,1] [,2] [,3] ## [1,] 5 11 17 # or equivalently crossprod(X, W) ## [,1] [,2] [,3] ## [1,] 5 11 17 3.2.3 Implementing 3-layer neural network identity_func &lt;- function(a) { a } init_network &lt;- function() { list(W1 = array((1:6)/10, dim=c(2,3)), b1 = (1:3)/10, W2 = array((1:6)/10, dim=c(3,2)), b2 = c(0.1, 0.2), W3 = array((1:4)/10, dim=c(2,2)), b3 = c(0.1, 0.2)) } forward &lt;- function(network, x) { if (is.vector(x)) x &lt;- array(x, dim=c(1, length(x))) a1 &lt;- x %*% network$W1 + network$b1 z1 &lt;- sigmoid(a1) a2 &lt;- z1 %*% network$W2 + network$b2 z2 &lt;- sigmoid(a2) a3 &lt;- z2 %*% network$W3 + network$b3 y &lt;- identity_func(a3) y } network &lt;- init_network() x &lt;- c(1, 0.5) y &lt;- forward(network, x) y ## [,1] [,2] ## [1,] 0.3168271 0.6962791 3.3 Output layer softmax &lt;- function(a) { C &lt;- max(a) exp_a &lt;- exp(a-C) exp_a / sum(exp_a) } a &lt;- c(0.3, 2.9, 4.0) y &lt;- softmax(a) y ## [1] 0.01821127 0.24519181 0.73659691 sum(y) ## [1] 1 3.4 MNIST Run scripts for defining the mnist data loader and image plot helper functions. source(&quot;mnist.R&quot;) source(&quot;helpers.R&quot;) d &lt;- load_mnist(normalize=TRUE, flatten=FALSE, one_hot_label=FALSE) str(d) ## List of 4 ## $ train_img : num [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ... ## $ train_label: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ... ## $ test_img : num [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ... ## $ test_label : int [1:10000] 7 2 1 0 4 1 4 9 5 9 ... random_plot(d$train_img, d$train_label) random_plot(d$test_img, d$test_label) 3.4.1 Convert pickled sample weights to R data The original book provides pre-trained network weights in the GitHub repository sample_weight.pkl. The file needs to be converted R data format to be used in R. To do so, first, use savetxt method of numpy to save weights in text files. The python script below creates six text files: b1, b2, b3, W1, W2 and W3. import numpy as np import pickle with open(&quot;sample_weight.pkl&quot;, &quot;rb&quot;) as f: data = pickle.load(f) for key in data: np.savetxt(key, data[key]) Then run the following command in R to make them into a R data file. b1 &lt;- scan(&quot;sample-weight/b1&quot;) b2 &lt;- scan(&quot;sample-weight/b2&quot;) b3 &lt;- scan(&quot;sample-weight/b3&quot;) W1 &lt;- read.table(&quot;sample-weight/W1&quot;, header=FALSE, sep=&quot; &quot;) W2 &lt;- read.table(&quot;sample-weight/W2&quot;, header=FALSE, sep=&quot; &quot;) W3 &lt;- read.table(&quot;sample-weight/W3&quot;, header=FALSE, sep=&quot; &quot;) W1 &lt;- as.matrix(W1) W2 &lt;- as.matrix(W2) W3 &lt;- as.matrix(W3) dimnames(W1) &lt;- NULL dimnames(W2) &lt;- NULL dimnames(W3) &lt;- NULL out &lt;- list(b1=b1, b2=b2, b3=b3, W1=W1, W2=W2, W3=W3) saveRDS(out, &quot;sample_weight.rds&quot;) 3.4.2 Prediction with sample network weights get_data &lt;- function() { # load and return test data d &lt;- load_mnist(normalize=TRUE, flatten=TRUE, one_hot_label=FALSE) d[c(&quot;test_img&quot;, &quot;test_label&quot;)] } init_network &lt;- function() { # load and return network weights network &lt;- readRDS(&quot;sample_weight.rds&quot;) network } prediction &lt;- function(network, x) { if (is.vector(x)) x &lt;- array(x, dim=c(1, length(x))) a1 &lt;- x %*% network$W1 + network$b1 z1 &lt;- sigmoid(a1) a2 &lt;- z1 %*% network$W2 + network$b2 z2 &lt;- sigmoid(a2) a3 &lt;- z2 %*% network$W3 + network$b3 y &lt;- softmax(a3) y } d &lt;- get_data() network &lt;- init_network() str(d) ## List of 2 ## $ test_img : num [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ... ## $ test_label: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ... str(network) ## List of 6 ## $ b1: num [1:50] -0.0675 0.0696 -0.0273 0.0226 -0.22 ... ## $ b2: num [1:100] -0.01471 -0.07215 -0.00156 0.122 0.11603 ... ## $ b3: num [1:10] -0.06024 0.00933 -0.0136 0.02167 0.01074 ... ## $ W1: num [1:784, 1:50] -0.00741 -0.0103 -0.01309 -0.01001 0.02207 ... ## $ W2: num [1:50, 1:100] -0.1069 0.2991 0.0658 0.0939 0.048 ... ## $ W3: num [1:100, 1:10] -0.422 -0.524 0.683 0.155 0.505 ... One-by-one prediction. accuracy_count &lt;- 0 for (i in 1:length(d$test_label)) { y &lt;- prediction(network, d$test_img[i,]) p &lt;- which.max(y)-1L if (p == d$test_label[i]) accuracy_count &lt;- accuracy_count + 1 } cat(&quot;Accuracy:&quot;, accuracy_count/length(d$test_label), &quot;\\n&quot;) ## Accuracy: 0.9352 For batch prediction, we need to modify the softmax function to work for matrix input. softmax &lt;- function(a) { # a : either numeric vector or matrix of size (N, classes) # # returns: probability matrix of size (N, classes) if (is.vector(a)) dim(a) &lt;- c(1, length(a)) C &lt;- max(a) exp_a &lt;- exp(a-C) exp_a / rowSums(exp_a) } a &lt;- matrix(c(3, 1, 2, 1, 2, 5), nrow=2, ncol=3, byrow=TRUE) softmax(a) ## [,1] [,2] [,3] ## [1,] 0.66524096 0.09003057 0.2447285 ## [2,] 0.01714783 0.04661262 0.9362396 rowSums(softmax(a)) ## [1] 1 1 Also, prediction function needs to be revised due to the broadcasting rule of R. prediction &lt;- function(network, x) { if (is.vector(x)) x &lt;- array(x, dim=c(1, length(x))) x &lt;- t(x) a1 &lt;- crossprod(network$W1, x) + network$b1 z1 &lt;- sigmoid(a1) a2 &lt;- crossprod(network$W2, z1) + network$b2 z2 &lt;- sigmoid(a2) a3 &lt;- crossprod(network$W3, z2) + network$b3 a3 &lt;- t(a3) y &lt;- softmax(a3) y } Batch prediction. batch_size &lt;- 100 accuracy_count &lt;- 0 for (i in seq(1, length(d$test_label), batch_size)) { index &lt;- i:min((i+batch_size-1), length(d$test_label)) y &lt;- prediction(network, d$test_img[index,]) p &lt;- apply(y, 1, which.max) - 1L # one-base index accuracy_count &lt;- accuracy_count + sum(p==d$test_label[index]) } cat(&quot;Accuracy:&quot;, accuracy_count/length(d$test_label), &quot;\\n&quot;) ## Accuracy: 0.9352 "],
["training-neural-network.html", "4 Training Neural Network 4.1 Loss functions 4.2 Numerical Differentiation 4.3 Gradient 4.4 Gradient Descent", " 4 Training Neural Network This chapter corresponds to Chapter 4, “Training Neural Network” in the original book. 4.1 Loss functions Following implementation of loss functions allow for batch input. mean_squared_error &lt;- function(p, y) { # p: predicted value matrix (N, K) # y: true value matrix (N, K) if (is.vector(p)) dim(p) &lt;- c(1, length(p)) if (is.vector(y)) dim(y) &lt;- c(1, length(y)) 0.5*sum((p-y)^2)/nrow(p) } y &lt;- c(0,0,1,0,0,0,0,0,0,0) p &lt;- c(0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0) mean_squared_error(p, y) ## [1] 0.0975 p &lt;- c(0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0) mean_squared_error(p, y) ## [1] 0.5975 cross_entropy_error &lt;- function(p, y) { if (is.vector(p)) dim(p) &lt;- c(1, length(p)) if (is.vector(y)) dim(y) &lt;- c(1, length(y)) -sum(y * log(p + 1e-7)) / nrow(p) } y &lt;- c(0,0,1,0,0,0,0,0,0,0) p &lt;- c(0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0) cross_entropy_error(p, y) ## [1] 0.5108255 p &lt;- c(0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0) cross_entropy_error(p, y) ## [1] 2.302584 4.2 Numerical Differentiation numerical_diff &lt;- function(f, x) { h &lt;- 1e-4 (f(x+h) - f(x-h))/(2*h) } func1 &lt;- function(x) { 0.01*x^2 + 0.1*x } x &lt;- seq(0, 20, 0.1) y &lt;- func1(x) library(ggplot2) qplot(x, y) numerical_diff(func1, 5) ## [1] 0.2 numerical_diff(func1, 10) ## [1] 0.3 g &lt;- qplot(x, y) slope &lt;- numerical_diff(func1, 5) icept &lt;- func1(5) - slope*5 g1 &lt;- g + geom_abline(slope=slope, intercept=icept, linetype=2) slope &lt;- numerical_diff(func1, 10) icept &lt;- func1(10) - slope*10 g2 &lt;- g + geom_abline(slope=slope, intercept=icept, linetype=2) library(gridExtra) grid.arrange(g1, g2, ncol=2) 4.3 Gradient func2 &lt;- function(x) { x[1]^2 + x[2]^2 } x1 &lt;- seq(-3, 3, 0.5) x2 &lt;- seq(-3, 3, 0.5) d &lt;- expand.grid(x1=x1, x2=x2) d$y &lt;- apply(d, 1, func2) z &lt;- matrix(d$y, nrow=length(x1), ncol=length(x2)) persp(x1, x2, z, theta=30, phi = 40, expand = 0.7, col = &quot;grey&quot;, ticktype=&quot;detailed&quot;) numeric_gradient &lt;- function(f, x) { h &lt;- 1e-4 if (!is.array(x)) x &lt;- array(x) grad &lt;- array(0, dim=dim(x)) for (i in seq_along(x)) { tmp &lt;- x[i] x[i] &lt;- tmp + h f1 &lt;- f(x) x[i] &lt;- tmp - h f2 &lt;- f(x) grad[i] &lt;- (f1-f2) / (2*h) x[i] &lt;- tmp } grad } numeric_gradient(func2, c(3, 4)) ## [1] 6 8 numeric_gradient(func2, c(0, 2)) ## [1] 0 4 numeric_gradient(func2, c(6, 0)) ## [1] 12 0 x1 &lt;- seq(-3, 3, 0.5) x2 &lt;- seq(-3, 3, 0.5) d &lt;- expand.grid(x1=x1, x2=x2) d$y &lt;- apply(d, 1, func2) d$d1 &lt;- -2*d$x1 d$d2 &lt;- -2*d$x2 ggplot(d, aes(x1, x2, xend=x1+d1/20, yend=x2+d2/20)) + geom_segment(arrow=arrow(length=unit(0.1, &quot;cm&quot;))) 4.4 Gradient Descent gradient_descent &lt;- function(f, init_x, lr=0.1, step_num=100) { x &lt;- init_x for (i in 1:step_num) { grad &lt;- numeric_gradient(f, x) x &lt;- x-lr*grad } x } init_x &lt;- c(-3, 4) gradient_descent(func2, init_x) ## [1] -6.111108e-10 8.148144e-10 gradient_descent_plot &lt;- function(f, init_x, lr=0.1, step_num=100) { x &lt;- init_x out &lt;- x for (i in 1:step_num) { grad &lt;- numeric_gradient(f, x) x &lt;- x-lr*grad out &lt;- rbind(out, x) } dimnames(out) &lt;- NULL out &lt;- as.data.frame(out) names(out) &lt;- c(&quot;x1&quot;, &quot;x2&quot;) x1 &lt;- seq(-5, 5, 0.2) x2 &lt;- seq(-5, 5, 0.2) d &lt;- expand.grid(x1=x1, x2=x2) d$y &lt;- apply(d, 1, f) ggplot(d, aes(x1, x2)) + geom_contour(aes(z=y)) + geom_point(data=out, aes(x1, x2)) + xlim(c(-5, 5)) + ylim(c(-5, 5)) } init_x &lt;- c(-3, 4) gradient_descent_plot(func2, init_x) gradient_descent_plot(func2, init_x, lr=10) ## Warning: Removed 100 rows containing missing values (geom_point). gradient_descent_plot(func2, init_x, lr=1e-10) "]
]
